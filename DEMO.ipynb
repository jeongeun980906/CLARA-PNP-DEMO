{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import flax\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import os\n",
    "from flax.training import checkpoints\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import imageio\n",
    "import openai\n",
    "import copy\n",
    "import cv2\n",
    "\n",
    "from env.env import PICK_TARGETS, PLACE_TARGETS, PickPlaceEnv\n",
    "import clip\n",
    "import torch\n",
    "from moviepy.editor import ImageSequenceClip\n",
    "from clipport.model import TransporterNets, n_params\n",
    "from clipport.train import train_step, eval_step\n",
    "from clipport.run import run_cliport\n",
    "\n",
    "from llm.score import gpt3_scoring, make_options\n",
    "from llm.helper import *\n",
    "from llm.affordance import affordance_scoring, affordance_score2\n",
    "from llm.davnici import lm_planner_unct\n",
    "from llm.run import run\n",
    "from vild.forward import vild"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = PickPlaceEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_input = \"put one block in red bowl.\" \n",
    "config = {\"pick\":  [\"red block\", \"yellow block\", \"blue block\"],\n",
    "          \"place\": ['green bowl', 'red bowl']}\n",
    "\n",
    "termination_string = \"done()\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.reset(config)\n",
    "\n",
    "img_top = env.get_camera_image()\n",
    "img_top_rgb = cv2.cvtColor(img_top, cv2.COLOR_BGR2RGB)\n",
    "plt.imshow(img_top)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_model, clip_preprocess = clip.load(\"ViT-B/32\")\n",
    "clip_model.cuda().eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coord_x, coord_y = np.meshgrid(np.linspace(-1, 1, 224), np.linspace(-1, 1, 224), sparse=False, indexing='ij')\n",
    "coords = np.concatenate((coord_x[..., None], coord_y[..., None]), axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_names = ['blue block',\n",
    "                  'red block',\n",
    "                  'green block',\n",
    "                  'orange block',\n",
    "                  'yellow block',\n",
    "                  'purple block',\n",
    "                  'pink block',\n",
    "                  'cyan block',\n",
    "                  'brown block',\n",
    "                  'gray block',\n",
    "\n",
    "                  'blue bowl',\n",
    "                  'red bowl',\n",
    "                  'green bowl',\n",
    "                  'orange bowl',\n",
    "                  'yellow bowl',\n",
    "                  'purple bowl',\n",
    "                  'pink bowl',\n",
    "                  'cyan bowl',\n",
    "                  'brown bowl',\n",
    "                  'gray bowl']\n",
    "\n",
    "#@markdown ViLD settings.\n",
    "category_name_string = \";\".join(category_names)\n",
    "max_boxes_to_draw = 8 #@param {type:\"integer\"}\n",
    "\n",
    "# Extra prompt engineering: swap A with B for every (A, B) in list.\n",
    "prompt_swaps = [('block', 'cube')]\n",
    "\n",
    "nms_threshold = 0.4 #@param {type:\"slider\", min:0, max:0.9, step:0.05}\n",
    "min_rpn_score_thresh = 0.4  #@param {type:\"slider\", min:0, max:1, step:0.01}\n",
    "min_box_area = 10 #@param {type:\"slider\", min:0, max:10000, step:1.0}\n",
    "max_box_area = 3000  #@param {type:\"slider\", min:0, max:10000, step:1.0}\n",
    "vild_params = max_boxes_to_draw, nms_threshold, min_rpn_score_thresh, min_box_area, max_box_area"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ViLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = \"./2db.png\"\n",
    "np.random.seed(2)\n",
    "if config is None:\n",
    "  pick_items = list(PICK_TARGETS.keys())\n",
    "  pick_items = np.random.choice(pick_items, size=np.random.randint(1, 5), replace=False)\n",
    "\n",
    "  place_items = list(PLACE_TARGETS.keys())[:-9]\n",
    "  place_items = np.random.choice(place_items, size=np.random.randint(1, 6 - len(pick_items)), replace=False)\n",
    "  config = {\"pick\":  pick_items,\n",
    "            \"place\": place_items}\n",
    "  print(pick_items, place_items)\n",
    "\n",
    "obs = env.reset(config)\n",
    "\n",
    "img_top = env.get_camera_image_top()\n",
    "img_top_rgb = cv2.cvtColor(img_top, cv2.COLOR_BGR2RGB)\n",
    "plt.imshow(img_top)\n",
    "\n",
    "imageio.imsave(image_path, img_top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vild_model = vild(clip_model,category_name_string, vild_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "found_objects, boxes = vild_model.infer(image_path,plot_on=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_planner = lm_planner_unct(2)\n",
    "lm_planner.objects = copy.deepcopy(found_objects)\n",
    "lm_planner.set_goal(raw_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "THRES = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "done = False\n",
    "num_tasks = 0\n",
    "max_tasks = 5\n",
    "steps_text = []\n",
    "while not done:\n",
    "    num_tasks += 1\n",
    "    if num_tasks > max_tasks:\n",
    "        break\n",
    "    tasks, scores , unct = lm_planner.plan_with_unct()\n",
    "    for t in tasks:\n",
    "        if 'done' in t:\n",
    "            done = True\n",
    "            break\n",
    "    if done:\n",
    "        break\n",
    "    if tasks != None:\n",
    "        selected_task = None\n",
    "        if len(scores)>0:\n",
    "            scores = np.asarray(scores)\n",
    "            idxs= np.argsort(scores)\n",
    "            flag = False\n",
    "            for idx in idxs[::-1]:\n",
    "                try:\n",
    "                    aff = affordance_score2(tasks[idx], found_objects)\n",
    "                except:\n",
    "                    print(tasks[idx])\n",
    "                    aff = 0\n",
    "                if aff > 0:\n",
    "                    selected_task = tasks[idx]\n",
    "                    lm_planner.append(None, None, selected_task)\n",
    "                    break\n",
    "            if aff == 2: \n",
    "                done=True \n",
    "                lm_planner.append(None, None, tasks[idx])\n",
    "                # steps_text.append(\"done()\")\n",
    "                # uncts.append(unct)\n",
    "                break\n",
    "        if unct > THRES:\n",
    "            reason, ques = lm_planner.question_generation()\n",
    "            answer = input(\"Answer: \")\n",
    "            lm_planner.answer(answer)\n",
    "        elif selected_task != None:\n",
    "            steps_text.append(selected_task)\n",
    "    else: break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model weights using dummy tensors.\n",
    "rng = jax.random.PRNGKey(0)\n",
    "rng, key = jax.random.split(rng)\n",
    "init_img = jnp.ones((4, 224, 224, 5), jnp.float32)\n",
    "init_text = jnp.ones((4, 512), jnp.float32)\n",
    "init_pix = jnp.zeros((4, 2), np.int32)\n",
    "init_params = TransporterNets().init(key, init_img, init_text, init_pix)['params']\n",
    "# gpus = jax.devices('gpu')\n",
    "# init_params = jax.jit(init_params,device=gpus[1])\n",
    "print(f'Model parameters: {n_params(init_params):,}')\n",
    "optim = flax.optim.Adam(learning_rate=1e-4).create(init_params)\n",
    "\n",
    "\n",
    "ckpt_path = f'ckpt_{40000}'\n",
    "if not os.path.exists(ckpt_path):\n",
    "    !gdown --id 1Nq0q1KbqHOA5O7aRSu4u7-u27EMMXqgP\n",
    "optim = checkpoints.restore_checkpoint(ckpt_path, optim)\n",
    "print('Loaded:', ckpt_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_img(env, image_path):\n",
    "    img_top = env.get_camera_image_top()\n",
    "    imageio.imsave(image_path, img_top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Initial state:')\n",
    "plt.imshow(env.get_camera_image())\n",
    "for i, step in enumerate(steps_text[:-1]):\n",
    "  if step == '' or step == termination_string:\n",
    "    break\n",
    "  print(step)\n",
    "  nlp_step = step_to_nlp(step)\n",
    "  print('GPT-3 says next step:', nlp_step)\n",
    "  obs = run_cliport(env,clip_model,coords, optim, obs, nlp_step)\n",
    "  # success = False\n",
    "  # while not success:\n",
    "  #   obs = run_cliport(env,clip_model,coords, optim, obs, nlp_step)\n",
    "  #   save_img(env, image_path)\n",
    "  #   category_name_string = \";\".join(found_objects)\n",
    "  #   vild_model.category_name_string = copy.deepcopy(category_name_string)\n",
    "  #   found_objects, boxes = vild_model.infer(image_path)\n",
    "  #   success = success_detector(found_objects,boxes,step)\n",
    "\n",
    "  # Show camera image after task.\n",
    "print('Final state:')\n",
    "plt.imshow(env.get_camera_image())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug_clip = ImageSequenceClip(env.cache_video, fps=25)\n",
    "display(debug_clip.ipython_display(autoplay=1, loop=1, center=False))\n",
    "env.cache_video = []"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
